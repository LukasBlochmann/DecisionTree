# Submission No. 1 -- Decision Tree

The following documentation explains, how the data generated by this project is to be evaluated and used. All information needed to understand the "prophecy.py" script can be found in this document, in "util/HoT_Build-A-Decision-Tree.pdf" and "util/Class02.pptx". Please also note, that the answers to the tasks placed in "util/HoT_Build-A-Decision-Tree.pdf" can be found in "util/solution_for_submitting/anwers.pdf".

## Script Usage

In order to properly use the script the training data needs to follow a certain format (json dictionary format). One can define as many categories as he wants, but every data entry needs exactly the same categories. One category has do be defined mandatorily: **["decision": "yes/no"]**. Please mind that only the decisions "yes" and "no" are accectable at this time.

```
[
  {"category1": "parameter",   "category2": "parameter",   ...,   "decision": "yes"},
  {"category1": "parameter",   "category2": "parameter",   ...,   "decision": "no"},
]
```

For further understanding please look into the "verification" folder and its data sets.

Furthermore, the training data needs to be stored in a ".json" file which has to be named "training_data.json".

In the final step, the execution of the script, one only has to put the "prophecy.py" script and the "training_data.json" into the same folder and execute the script. The results will be generated automatically and the decision tree saved as "actual_result.png".

## Verification

In order to verify the functionality of the "prophecy.py" script, two examples were taken from the internet and manually calculated using the underlying script. You can find these verifications in the "verification" folder. Every verification example consists of a data set ("training_data.json") and the expected result ("expected_result.png"). The images displaying the expected result were taken from the specific websites. During the verification process, the "prophecy.py" script from the "build" folder was used to generate its own results. The script was successfully verified with both verification examples.

#### 1. Example 

The first example can be found in "verification/ver01". It was taken from [here](https://datahacker.rs/011-machine-learning-decision-three/).

<!-- Expected result from the website:
![first_example_ver](/verification/ver01/expected_result.png) -->

Result generated by "prophecy.py":
![first_example_out](/verification/ver01/generated_result.png)

#### 2. Example

The second example can be found in "verification/ver01". It was taken from [here](https://medium.com/@MrBam44/decision-trees-91f61a42c724).

<!-- Expected result from the website:
![first_example_ver](/verification/ver02/expected_result.png) -->

Result generated by "prophecy.py":
![first_example_out](/verification/ver02/generated_result.png)



## Reading the .log file 

The "prophecy.py" script generates a "calculation_results.log" file. This file contains all relevant information to draw the final decision tree. Furthermore, it clearly supports the reader to understand the logic behind choosing the right roots and leafs. The Log-file can be divided into two sections.

### Section.1: Initial Data Calculations

This section calculates all the needed initial data and creates the backbone for the decision tree structure.
How the different values are calculated can be looked up through the comments in "prophecy.py" or the "util/HoT_Build-A-Decision-Tree.pdf" file. However, this section represents the first iteration of the recursive calculations 
(see "util/HoT_Build-A-Decision-Tree.pdf", sections 1.1.4.a - 1.1.4.d). 

### Section.2: Decision Tree Build Process

The second section actually resembles the decision tree building process. The algorithm calculates all subtrees recursively with the root node as the first the different leaf nodes the last function calls. So the tree builds itself in a "bottom-up" process. 

The log-entry "Valid roots: [(...)]" is needed, when multiple categories from Section.1 have the same and highest gain. The the algorithm just chooses alphabetically. However, this entry is important if one wants to manually look for alternative roots (e.g. if the tree only consists of two branches like in the underlying data set from "util/HoT_Build-A-Decision-Tree.pdf").

After the root was chosen, the indentation gives information about the current tree depth, with root at "indentation = 0".
Please mind that nodes are marked with "<node_name>" and leafs with "(leaf_name)".
In case the result of a branch is another node, the following information is given:

Example:
```
Branch: <outlook : sunny>
Entropy for remaining dataset fulfilling <outlook : sunny>: 0.970951
Result: <wind> -- Gain on remaining dataset: 0.970951.
```

Explanation:
```
Branch: <outlook : sunny>
```
- Branch: <start_node : branch_parameter>

```
Entropy for remaining dataset fulfilling <outlook : sunny>: 0.970951
```
- The subtree under a specific node must only contain the data, that fits the constraints of the start node and the branch parameter. So another data entry, fulfilling for example "<outlook : rainy>", will not be included in this iteration.

```
Result: <wind> -- Gain on remaining dataset: 0.970951.
```
- Wind is the category with the highest gain on the new dataset (new dataset excludes all nodes used up to this point).